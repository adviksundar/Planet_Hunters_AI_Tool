{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Planet Hunters Notebook 3"
      ],
      "metadata": {
        "id": "ThH8jGwChKt3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0ygrZd3dmu5"
      },
      "source": [
        "# Classifying Exoplanets\n",
        "\n",
        "In this notebook, we'll continue improving our models for exoplanet classification!\n",
        "\n",
        "We'll be:\n",
        "*   Preprocessing the Dataset similar to before\n",
        "*   Implementing more modern and complex machine learning architectures to see which one performs best!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irPDgkzsdguU"
      },
      "source": [
        "## Exoplanet Classification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Previously, we were able to visualize and augment the dataset from Kepler. Now that we better understand the data that we're working with, we can begin to dive into more complex architectures to classify exoplanet stars, and the difficulties faced when doing so."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmzc3C9XkIKL"
      },
      "source": [
        "**IMPORTANT**: We have to make sure we've got 'Change Runtime Type' set to **GPU** in Colab!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjzYKqcweS2F",
        "cellView": "form"
      },
      "source": [
        "#@title Run this code to get started\n",
        "%tensorflow_version 2.x\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTrain.csv'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Planet%20Hunters/exoTest.csv'\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import  metrics\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.signal import savgol_filter\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score,plot_confusion_matrix,precision_score,recall_score,f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, normalize\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling2D, BatchNormalization, MaxPooling1D\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "df_train = pd.read_csv('exoTrain.csv')\n",
        "df_train.LABEL = df_train.LABEL -1\n",
        "df_test = pd.read_csv('exoTest.csv')\n",
        "df_test.LABEL = df_test.LABEL - 1\n",
        "\n",
        "def plot_graphs(history, best):\n",
        "\n",
        "  plt.figure(figsize=[10,4])\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy across training\\n best accuracy of %.02f'%best[1])\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss across training\\n best loss of %.02f'%best[0])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def analyze_results(model, train_X, train_y, test_X, test_y):\n",
        "    \"\"\"\n",
        "    Helper function to help interpret and model performance.\n",
        "\n",
        "    Args:\n",
        "    model: estimator instance\n",
        "    train_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model training.\n",
        "    train_y : array-like of shape (n_samples,)\n",
        "    Target values for model training.\n",
        "    test_X: {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "    Input values for model testing.\n",
        "    test_y : array-like of shape (n_samples,)\n",
        "    Target values for model testing.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    print(\"-------------------------------------------\")\n",
        "    print(\"Model Results\")\n",
        "    print(\"\")\n",
        "    print(\"Training:\")\n",
        "    if type(model) == keras.engine.sequential.Sequential:\n",
        "      train_predictions = model.predict(train_X)\n",
        "      train_predictions = (train_predictions > 0.5)\n",
        "      cm = confusion_matrix(train_y, train_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - TestData')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      fig = plt.figure(figsize=(22,7))\n",
        "      ax = fig.add_subplot(1,3,1)\n",
        "      plot_confusion_matrix(model,train_X,train_y,ax=ax,values_format = '.0f')\n",
        "      plt.show()\n",
        "    print(\"Testing:\")\n",
        "    if type(model) == keras.engine.sequential.Sequential:\n",
        "      test_predictions = model.predict(test_X)\n",
        "      test_predictions = (test_predictions > 0.5)\n",
        "      cm = confusion_matrix(test_y, test_predictions)\n",
        "      labels = [0, 1]\n",
        "      df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "      fig = plt.figure()\n",
        "      res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "      #plt.yticks([1.25, 3.75], labels,va='center')\n",
        "      plt.title('Confusion Matrix - TestData')\n",
        "      plt.ylabel('True label')\n",
        "      plt.xlabel('Predicted label')\n",
        "      plt.show()\n",
        "    else:\n",
        "      fig = plt.figure(figsize=(22,7))\n",
        "      ax = fig.add_subplot(1,3,1)\n",
        "      plot_confusion_matrix(model,test_X,test_y,ax=ax,values_format = '.0f')\n",
        "      plt.show()\n",
        "\n",
        "def reset(train,test):\n",
        "    train_X = train.drop('LABEL', axis=1)\n",
        "    train_y = train['LABEL'].values\n",
        "    test_X = test.drop('LABEL', axis=1)\n",
        "    test_y = test['LABEL'].values\n",
        "    return train_X,train_y,test_X,test_y\n",
        "\n",
        "train_X,train_y,test_X,test_y = reset(df_train, df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAc67kxQftEV"
      },
      "source": [
        "Note that `df_train` and `df_test` are the Pandas data frames that store our training and test datapoints. Similar to before, we'll now augment the data before exploring more modern, complex machine learning architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6yRtFD1pYP7d"
      },
      "source": [
        "#@title Run this code to preprocess data\n",
        "# Helper functions that we can run for the three augmentation functions that will be used, but not explroed in depth\n",
        "\n",
        "def smote(a,b):\n",
        "    model = SMOTE()\n",
        "    X,y = model.fit_sample(a, b)\n",
        "    return X,y\n",
        "\n",
        "def savgol(df1,df2):\n",
        "    x = savgol_filter(df1,21,4,deriv=0)\n",
        "    y = savgol_filter(df2,21,4,deriv=0)\n",
        "    return x,y\n",
        "\n",
        "def fourier(df1,df2):\n",
        "    train_X = np.abs(np.fft.fft(df1, axis=1))\n",
        "    test_X = np.abs(np.fft.fft(df2, axis=1))\n",
        "    return train_X,test_X\n",
        "\n",
        "def norm(df1,df2):\n",
        "    train_X = normalize(df1)\n",
        "    test_X = normalize(df2)\n",
        "    return train_X,test_X\n",
        "\n",
        "def robust(df1,df2):\n",
        "    scaler = RobustScaler()\n",
        "    train_X = scaler.fit_transform(df1)\n",
        "    test_X = scaler.transform(df2)\n",
        "    return train_X,test_X\n",
        "\n",
        "fourier_train_X, fourier_test_X = fourier(train_X, test_X)\n",
        "savgol_train_X, savgol_test_X = savgol(fourier_train_X, fourier_test_X)\n",
        "norm_train_X, norm_test_X = norm(savgol_train_X,savgol_test_X)\n",
        "robust_train_X, robust_test_X = robust(norm_train_X, norm_test_X)\n",
        "smote_train_X,smote_train_y = smote(robust_train_X, train_y)\n",
        "\n",
        "# Here we're adding the generated, augmented data onto the testing data\n",
        "aug_train_X, new_X_test_data, aug_train_y, new_y_test_data = train_test_split(smote_train_X, smote_train_y, test_size=0.3)\n",
        "aug_test_X = np.concatenate((robust_test_X, new_X_test_data), axis=0)\n",
        "aug_test_y = np.concatenate((test_y, new_y_test_data), axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRQPCRgpZVpW"
      },
      "source": [
        "Awesome! Now we'll have access to the augmented dataset as `aug_train_X`, `aug_text_X`, `aug_train_y`, and `aug_test_y`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw7FwS0_W7Bx"
      },
      "source": [
        "## MLP\n",
        "\n",
        "Let's start with neural nets!\n",
        "\n",
        "MLP stands for Multi-layer Perceptron, a specific kind of simple neural network. Thankfully, this is something that Sklearn supports, and it's already imported as MLPClassifier.\n",
        "\n",
        "\n",
        "![visual](https://s3.amazonaws.com/stackabuse/media/intro-to-neural-networks-scikit-learn-3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmbSCBCH2I9p"
      },
      "source": [
        "#### Step 1: Create our model\n",
        "\n",
        "We'll complete this by using a `MLPClassifier` model imported by the sklearn package. We can view the original documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). We'll create a model with:\n",
        "1. One hidden layer with 10 units\n",
        "2. random_state = 1\n",
        "3. 300 max iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBW-7S3mHVyA"
      },
      "source": [
        "# Create an MLP model (will train later)\n",
        "\n",
        "model = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXj7fzfkXuQ9"
      },
      "source": [
        "Now, we'll train our model using `aug_train_X` and `aug_train_y`, and analyze its accuracy and confusion matrix!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nFtWOLkXx5N"
      },
      "source": [
        "model.fit(aug_train_X, aug_train_y)\n",
        "\n",
        "train_predictions = model.predict(aug_train_X)\n",
        "test_predictions = model.predict(aug_test_X)\n",
        "print(accuracy_score(aug_train_y, train_predictions))\n",
        "print(accuracy_score(aug_test_y, test_predictions))\n",
        "analyze_results(model=model, train_X=aug_train_X, train_y=aug_train_y, test_X=aug_test_X, test_y=aug_test_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRF34weGoClK"
      },
      "source": [
        "## Neural Networks (Tensorflow and Keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbCq5LpQgVkF"
      },
      "source": [
        "Now we'll do what we did before, but using `tensorflow` and `keras`. These libraries will be crucial as they will allow us to create more complex models.\n",
        "\n",
        "We'll start by creating a similar model using these new packages.\n",
        "\n",
        "We'll be using a `Sequential` model in order to act as a \"list of layers\", which we will define to match our previous example. Later, we'll use it to build more complex, advanced models. More information can be found [here](https://keras.io/api/layers/).\n",
        "\n",
        "1. We'll add a `Dense` layer with 10 hidden units and a ReLU activation function. This layer also requires an `input_shape` parameter.\n",
        "\n",
        "2. We'll add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXvobnHRJg7L"
      },
      "source": [
        "# Create an MLP model (will train later)\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "# then we add a \"Dense\" (i.e. fully connected) layer\n",
        "model.add(Dense(10, input_shape=(3197,), activation = \"relu\")) # for the first layer we specify the input dimensions\n",
        "# we end by defining the output layer, which has the number of dimensions of the predictions we're making\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sriqCvylFPSE"
      },
      "source": [
        "Now we will run the code block below to check the details of our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUdk-_M3FTqA"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QaGfpePBSKS"
      },
      "source": [
        "Now we will train and analyze our `model` like we did before! We'll need to specify these parameters to `fit`:\n",
        "\n",
        "1. `batch_size` = 64\n",
        "2. `epochs` = 20\n",
        "3. `verbose` = 1\n",
        "4. `validation_data` = (aug_test_X, aug_test_y)\n",
        "5. `shuffle` = True\n",
        "\n",
        "We'll save the history of the model as it trains or \"fits\" the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cu7wdxOBc3z"
      },
      "source": [
        "# Train and analyze the model\n",
        "\n",
        "# Train the model, see accuracies, and analyze the results\n",
        "\n",
        "#training the model\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "validation_data = (aug_test_X, aug_test_y)\n",
        "verbose = 1\n",
        "shuffle = True\n",
        "\n",
        "history = model.fit(aug_train_X, aug_train_y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                            validation_data=validation_data, shuffle=shuffle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIBRY5RnbrMY"
      },
      "source": [
        "Now we will see how to view the performance of the model as it trained over time!\n",
        "\n",
        "In addition, we still want to be able to plot the confusion matrix of the model to check for performance and potential class biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EV8K0HxgKQTt"
      },
      "source": [
        "performance = model.evaluate(aug_test_X, aug_test_y, batch_size=batch_size)\n",
        "plot_graphs(history, performance)\n",
        "\n",
        "analyze_results(model=model, train_X=aug_train_X, train_y=aug_train_y, test_X=aug_test_X, test_y=aug_test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA_rRs1jTjDw"
      },
      "source": [
        "## Convolutional Neural Network (CNN)\n",
        "\n",
        "One potential fault of our previous approach is memorizing the placement of specific patterns in the data. Although we were able to achieve great levels of accuracy, we might benefit from an architecture that can make decisions based on patterns no matter where they occur in the sample - for example, if we started measuring flux earlier or later!\n",
        "\n",
        "This is something that CNNs excel at. Most CNN architectures are set up to work with two dimensional inputs such as images, so our approach will be a bit different in working with and creating a one-dimensional CNN. However, similar concepts apply as we'll be passing a filter accoss the each data point with respect to time.\n",
        "\n",
        "[Here](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) is a link to learn more about convolutional neural nets, and [here's](https://poloclub.github.io/cnn-explainer/) an interactive demo to explore. We can try talking through the image of a traditional CNN below!\n",
        "\n",
        "![](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnQlAZ043qOo"
      },
      "source": [
        "First, we'll have to \"reshape\" our augmented data into a shape that can be fed into a 1-dimensional CNN. We need to reshape the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogS5RYSsz5qN"
      },
      "source": [
        "cnn_aug_train_X = np.expand_dims(aug_train_X, axis=2)\n",
        "cnn_aug_test_X = np.expand_dims(aug_test_X, axis=2)\n",
        "cnn_aug_train_y = aug_train_y\n",
        "cnn_aug_test_y = aug_test_y\n",
        "\n",
        "cnn_train_X = np.expand_dims(train_X, axis=2)\n",
        "cnn_test_X = np.expand_dims(test_X, axis=2)\n",
        "cnn_train_y = train_y\n",
        "cnn_test_y = test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtHMC1fJWsrM"
      },
      "source": [
        "Time to see the new shapes of the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfWG_Vd7XgTG"
      },
      "source": [
        "print(cnn_aug_train_X.shape)\n",
        "print(cnn_aug_test_X.shape)\n",
        "print(cnn_aug_train_y.shape)\n",
        "print(cnn_aug_test_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5bXRoaCTIj"
      },
      "source": [
        "Awesome!\n",
        "\n",
        "Now, we'll be using a `Sequential` model to build up our CNN. Steps for building the architecture are listed below:\n",
        "\n",
        "1. Add a `Conv1D` layer with 8 output filters, kernal size of 5, relu activation function, and padding = 'same'. This layer also requires an `input_shape` parameter.\n",
        "\n",
        "2. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.\n",
        "\n",
        "3. Add a `Conv1D` layer with 16 output filters, kernal size of 3, relu activation function, and padding = 'same'.\n",
        "\n",
        "4. Add a `MaxPooling1D` layer with pool_size = 4, strides = 4, and padding = 'same'.\n",
        "\n",
        "5. Add a `Flatten` layer.\n",
        "\n",
        "6. Add a `Dense` layer with 1 hidden unit and a sigmoid activation function. (This will be our output layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5S5wXQMTnEb"
      },
      "source": [
        "# Create a CNN model (will train later)\n",
        "\n",
        "# First, we initialize our model\n",
        "model = Sequential()\n",
        "input_shape = [3197, 1]\n",
        "\n",
        "cnn_layer_1 = Conv1D(8, 5, activation='relu', input_shape=input_shape, padding='same')\n",
        "cnn_layer_2 = MaxPooling1D(pool_size=4, strides=4, padding='same')\n",
        "cnn_layer_3 = Conv1D(16, 3, activation='relu', padding='same')\n",
        "cnn_layer_4 = MaxPooling1D(pool_size=4, strides=4, padding='same')\n",
        "cnn_layer_5 = Flatten()\n",
        "cnn_layer_6 = Dense(1, activation='sigmoid')\n",
        "\n",
        "# then we add a \"Dense\" (i.e. fully connected) layer\n",
        "model.add(cnn_layer_1) # for the first layer we specify the input dimensions\n",
        "model.add(cnn_layer_2)\n",
        "model.add(cnn_layer_3)\n",
        "model.add(cnn_layer_4)\n",
        "model.add(cnn_layer_5)\n",
        "model.add(cnn_layer_6)\n",
        "# we end by defining the output layer, which has the number of dimensions of the predictions we're making\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbDT84eyDGHY"
      },
      "source": [
        "Now we train the model like we did before!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVUCehR5TvLB"
      },
      "source": [
        "# Train and analyze the model\n",
        "\n",
        "# Train the model, see accuracies, and analyze the results\n",
        "\n",
        "#training the model\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "validation_data = (cnn_aug_test_X, cnn_aug_test_y)\n",
        "verbose = 1\n",
        "shuffle = True\n",
        "\n",
        "history = model.fit(cnn_aug_train_X, cnn_aug_train_y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                            validation_data=validation_data, shuffle=shuffle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StKM_5W6uOYZ"
      },
      "source": [
        "Once again, let's analyze the model's performance over time and the final confusion matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPik55GguYUT"
      },
      "source": [
        "performance = model.evaluate(cnn_aug_test_X, cnn_aug_test_y, batch_size=batch_size)\n",
        "plot_graphs(history, performance)\n",
        "\n",
        "analyze_results(model=model, train_X=cnn_aug_train_X, train_y=cnn_aug_train_y, test_X=cnn_aug_test_X, test_y=cnn_aug_test_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzWrP7kYgdVo"
      },
      "source": [
        "## Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svt0LBR34_WT"
      },
      "source": [
        "So far, we visually analyzed and refined raw satellite data, and built a top-of the line model that accurately detects exoplanet stars vs. non-exoplanet stars. This is critical to exoplanet hunting because it allows planetary hunters to focus on studying the exoplanets we've discovered, and analyzing them for mass, habitability, etc.\n",
        "\n",
        "Remember that in our original dataset, exoplanet stars accounted for less than 1 % of all samples collected. In notebooks 2 and 3, we used machine learning to automatically identify likely exoplanet stars, dramatically reducing the time and effort needed to find them!\n",
        "\n",
        "This pipeline can be used to help aid the search of exoplanets for the incoming, raw data. It might even lead to new planetary discoveries as space exploration continues! Try exploring more raw, unprocessed NASA data [here](https://www.nasa.gov/kepler/education/getlightcurves).\n",
        "\n",
        "Of course, the more data, the better. This model and pipeline can be further improved with future iterations of new data and architectures. If anyone decides to go planet hunting, they can have fun on their new adventures!"
      ]
    }
  ]
}